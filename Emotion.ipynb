{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eWmOMRoLQ5Z3",
    "outputId": "aba90e7c-4832-4544-d221-547985f01a24"
   },
   "outputs": [],
   "source": [
    "#! git clone https://github.com/kumarankit1996/Dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "juo4dpfxRBrt"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "02IbCJ35RCP3",
    "outputId": "dad8b97b-5ba7-4e1e-b960-235fa8f32271"
   },
   "outputs": [],
   "source": [
    "#! ls Dataset/HackerEarth/hm_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "64YoW_LBRT9f",
    "outputId": "f8b78ca0-9b94-4eb3-fc40-652d18186192"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import io\n",
    "from keras.layers import concatenate\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Dropout,UpSampling2D, Dense, MaxPooling2D, BatchNormalization, Input, Flatten, Lambda\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "import os\n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "import itertools\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "#from google.colab import files\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing, model_selection\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "id": "8BjqudI0UGgb",
    "outputId": "5099d73d-f57f-4347-c47f-acbf3ed31e5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection u'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/ankit/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/ankit/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/ankit/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/ankit/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/ankit/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/ankit/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/ankit/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/ankit/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/ankit/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/ankit/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/ankit/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /Users/ankit/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/ankit/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/ankit/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/ankit/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/ankit/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/ankit/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/ankit/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/ankit/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2dyr3MN7RiWd"
   },
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(\"hm_train.csv\")\n",
    "test_data=pd.read_csv(\"hm_test.csv\")\n",
    "#train_data['cleaned_hm'][60320],test_data['cleaned_hm'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the train and test dataset for creating bag of words \n",
    "bigdata = pd.concat([train_data['cleaned_hm'], test_data['cleaned_hm']], ignore_index=True)\n",
    "#bigdata[60320],bigdata[60321]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RspPnOL8Rv2S"
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    # extract the alphabet from sentences\n",
    "    sentence = re.sub(\"[^A-Za-z]\", \" \", sentence)\n",
    "    # changing the case of sentence to lower case\n",
    "    sentence = sentence.lower()\n",
    "    # tokenizing words in sentences\n",
    "    sentence = word_tokenize(sentence)\n",
    "    # Converting to bi grams\n",
    "    #sentence=nltk.bigrams(sentence)\n",
    "    # defined the word lemmatizer object\n",
    "    lemma = WordNetLemmatizer()\n",
    "    # deleting the stop words and finding the lemmatize words\n",
    "    sentence = [lemma.lemmatize(word,wordnet.VERB) for word in sentence if word not in set(stopwords.words(\"english\"))]\n",
    "    # joining the words to form sentence\n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8N5h17hWe-8A"
   },
   "outputs": [],
   "source": [
    "# Converting the target into one hot vector\n",
    "def category(data,s=\"NN\"):\n",
    "    target=[]\n",
    "    for i in range(len(data)):\n",
    "        if data[i]=='bonding':\n",
    "            target.append(0)\n",
    "        elif data[i]=='achievement':\n",
    "            target.append(1)\n",
    "        elif data[i]=='affection':\n",
    "            target.append(2)\n",
    "        elif data[i]=='leisure':\n",
    "            target.append(3)\n",
    "        elif data[i]=='enjoy_the_moment':\n",
    "            target.append(4)\n",
    "        elif data[i]=='nature':\n",
    "            target.append(5)\n",
    "        elif data[i]=='exercise':\n",
    "            target.append(6)\n",
    "    if s==\"NN\":\n",
    "        target = to_categorical(target)\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DU9Y3ellRxWP"
   },
   "outputs": [],
   "source": [
    "# Cleaning the dataset\n",
    "x=[]\n",
    "for i in range(len(bigdata)):\n",
    "    x.append(clean_sentence(bigdata[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZKO991jUXbg"
   },
   "outputs": [],
   "source": [
    "# Counting the frequency of words\n",
    "count_vectorizer = CountVectorizer()\n",
    "C = count_vectorizer.fit_transform(x).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f6GeJqTiXO6Q",
    "outputId": "50863096-a802-466e-f5c8-9c7494e9c336"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40213, 19802), (60321, 19802))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf_vectorizer = TfidfVectorizer()\n",
    "#values = tfidf_vectorizer.fit_transform(x).toarray()\n",
    "\n",
    "# Creating the train and test set from bag of words\n",
    "X=C[:60321]\n",
    "test_x=C[60321:]\n",
    "test_x.shape,X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the target into one hot vector\n",
    "Y=category(train_data['predicted_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tyyVBHFyfcgD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40213, 19802)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-2sEV0uHZ2M8"
   },
   "outputs": [],
   "source": [
    "mc = ModelCheckpoint(filepath='2weights_v8.h5', monitor='val_loss',period=1,save_best_only=True,save_weights_only=True,mode='auto', verbose = 3)\n",
    "es = EarlyStopping(patience=1000, monitor='val_loss', min_delta=0.0005, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1054
    },
    "colab_type": "code",
    "id": "JYH6I21ZcSTL",
    "outputId": "34e6ab70-7458-4d6f-af07-8863c7fa85ba"
   },
   "outputs": [],
   "source": [
    "input_dim = len(X[0])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(150, activation = 'relu'))\n",
    "model.add(Dense(150, activation = 'relu'))\n",
    "model.add(Dense(200, activation = 'relu'))\n",
    "model.add(Dense(200, activation = 'relu'))\n",
    "model.add(Dense(7, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy' , optimizer =optimizers.Adadelta(lr=0.01) , metrics = ['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, Y, epochs = 200,callbacks=[mc,es], batch_size = 64,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('2weights_v8.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qcxAxdgIXj53"
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=np.argmax(predicted,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=[]\n",
    "for i in range(len(p)):\n",
    "    if p[i]==0:\n",
    "        predict.append('bonding')\n",
    "    elif p[i]==1:\n",
    "        predict.append('achievement')\n",
    "    elif p[i]==2:\n",
    "        predict.append('affection')\n",
    "    elif p[i]==3:\n",
    "        predict.append('leisure')\n",
    "    elif p[i]==4:\n",
    "        predict.append('enjoy_the_moment')\n",
    "    elif p[i]==5:\n",
    "        predict.append('nature')\n",
    "    elif p[i]==6:\n",
    "        predict.append('exercise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=np.array(predict).reshape(-1,1)\n",
    "uid=np.array(test_data['hmid']).reshape(-1,1)\n",
    "output=np.hstack((uid,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=pd.DataFrame(output)\n",
    "output.columns=['hmid','predicted_category']\n",
    "output.to_csv(\"output22.csv\",sep=\",\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=category(train_data['predicted_category'],\"SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(X,Y,test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# defining the SVM Model\n",
    "SVM_model=SVC(kernel='rbf',C=2,gamma=0.05)\n",
    "history=SVM_model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Erms \n",
    "def ERMS(Predicted,Actual):\n",
    "    total=0.0\n",
    "    counter=0\n",
    "    for i in range(0,len(Predicted)):\n",
    "        total+=math.pow((Actual[i]-Predicted[i]),2)\n",
    "        #count for match\n",
    "        if (int(np.around(Predicted[i])) == Actual[i]):\n",
    "            counter+=1\n",
    "    Accuracy=(counter*100/len(Predicted))\n",
    "    #print(counter)\n",
    "    Erms=math.sqrt(total/len(Predicted))\n",
    "    return Erms, Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_PredictedTarget=SVM_model.predict(test_x)\n",
    "#Erms, SVM_Accu=ERMS(SVM_PredictedTarget,test_y)\n",
    "#print(\"Erms:\",Erms)\n",
    "#print(\"SVM USPS Accu:\",SVM_Accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=SVM_PredictedTarget\n",
    "predict=[]\n",
    "for i in range(len(p)):\n",
    "    if p[i]==0:\n",
    "        predict.append('bonding')\n",
    "    elif p[i]==1:\n",
    "        predict.append('achievement')\n",
    "    elif p[i]==2:\n",
    "        predict.append('affection')\n",
    "    elif p[i]==3:\n",
    "        predict.append('leisure')\n",
    "    elif p[i]==4:\n",
    "        predict.append('enjoy_the_moment')\n",
    "    elif p[i]==5:\n",
    "        predict.append('nature')\n",
    "    elif p[i]==6:\n",
    "        predict.append('exercise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=np.array(predict).reshape(-1,1)\n",
    "uid=np.array(test_data['hmid']).reshape(-1,1)\n",
    "output=np.hstack((uid,predict))\n",
    "output=pd.DataFrame(output)\n",
    "output.columns=['hmid','cleaned_hm']\n",
    "output.to_csv(\"svmoutput2.csv\",sep=\",\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HackerEarth2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
